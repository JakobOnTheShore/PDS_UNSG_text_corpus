{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694a3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import re\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "#  Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3295b8",
   "metadata": {},
   "source": [
    "# 1. Read single image - based pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75492a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = Path(\"D:/uppsala/15. ds projects\")\n",
    "RAW_PDF_DIR = BASE_DIR / \"PDS_UNSG_TEXT_CORPUS\" / \"data\" / \"speeches\" / \"pdf\"\n",
    "TMP_DIR = BASE_DIR / \"tmp_data\"\n",
    "TMP_DIR.mkdir(exist_ok=True)  # auto create if not exists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd581fa",
   "metadata": {},
   "source": [
    "# 2. Convert two columns to one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e032a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_to_single_column(pdf_name, input_dir=RAW_PDF_DIR, output_dir=TMP_DIR, k2_path=None, timeout=600):\n",
    "    \"\"\"Convert a two-column PDF to single-column and save it in the tmp_data folder\"\"\"\n",
    "\n",
    "    # prepare input and output paths\n",
    "    input_pdf = input_dir / pdf_name\n",
    "    output_pdf = output_dir / pdf_name\n",
    "\n",
    "    # find k2pdfopt executable\n",
    "    exe = k2_path or shutil.which(\"k2pdfopt\") or shutil.which(\"k2pdfopt.exe\")\n",
    "    if not exe:\n",
    "        raise FileNotFoundError(\"k2pdfopt not found, please ensure it is installed.\")\n",
    "\n",
    "    # build command\n",
    "    cmd = [\n",
    "        exe, str(input_pdf),\n",
    "        \"-o\", str(output_pdf),\n",
    "        \"-mode\", \"copy\",# keep original quality\n",
    "        \"-col\", \"2\", #  input = two columns\n",
    "        \"-dev\", \"pdf\",#  output format =  PDF\n",
    "        \"-ui-\", # close interactive UI mode\n",
    "        \"-p\", \"1-9999\", #  deal with pages 1 to 9999\n",
    "        \"-x\", #  removing white margins\n",
    "        \"-wrap-\", #  no  wrapping\n",
    "        \"-autorotate-\", #  no auto-rotation\n",
    "        \"-rt\", \"0\" #  no auto-rotation\n",
    "        \"-verbose\"\n",
    "\n",
    "\n",
    "    ]\n",
    "\n",
    "    # execute\n",
    "    res = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)\n",
    "    if res.returncode != 0:\n",
    "        raise RuntimeError(f\"k2pdfopt error: {res.stderr or res.stdout}\")\n",
    "\n",
    "    if not output_pdf.exists() or output_pdf.stat().st_size < 5000:\n",
    "        raise RuntimeError(f\"Output file error, please check: {output_pdf}\")\n",
    "\n",
    "    print(f\"✅ Conversion successful: {output_pdf.name} → Size {output_pdf.stat().st_size//1024} KB\")\n",
    "    return output_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55387dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_single_column(\"A_1983_38_PV.81_speeches.pdf\",k2_path=r\"D:\\LeStoreDownload\\k2pdfopt.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4268b533",
   "metadata": {},
   "source": [
    "# 3. Extract text using different techniques and calculate accuracy of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc79604",
   "metadata": {},
   "source": [
    "## Define the function of how to calculate text recognition accuracy\n",
    "* Using spaCy’s vocab to identify whether a word is valid (including verb tenses, plural forms, and common proper nouns)\n",
    "* If a word is capitalized and not at the beginning of a sentence, it is treated as a proper noun — no lemmatization and no validity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1733139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model for text evaluation\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"tagger\"])  # only used for lemmatization and vocab, no need for parser and tagger\n",
    "\n",
    "\n",
    "def evaluate_fast(text):\n",
    "    \"\"\"\n",
    "    Efficient text quality evaluation:\n",
    "    - complexity: O(n)\n",
    "    - leverage caching for validity checks,avoid lookups for every word\n",
    "    - output: accuracy, invalid-word dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    words = []\n",
    "    is_proper_flags = []\n",
    "    append_word = words.append\n",
    "    append_flag = is_proper_flags.append\n",
    "\n",
    "    # -----------❗ Query cache (greatly speeds up) -----------\n",
    "    valid_cache = {}  # word -> True/False\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_alpha:\n",
    "            continue\n",
    "\n",
    "        txt = token.text\n",
    "        lemma = token.lemma_\n",
    "\n",
    "        # Define \"proper noun\": capitalized and not sentence start\n",
    "        is_proper = txt[0].isupper() and not token.is_sent_start\n",
    "\n",
    "        # Normalize word form\n",
    "        if is_proper:\n",
    "            final = txt.lower()\n",
    "        else:\n",
    "            final = lemma.lower()\n",
    "\n",
    "        append_word(final)\n",
    "        append_flag(is_proper)\n",
    "\n",
    "    total = len(words)\n",
    "    if total == 0:\n",
    "        return 0.0, pd.DataFrame(columns=[\"invalid_word\", \"frequency\"])\n",
    "\n",
    "    invalid_words = []\n",
    "\n",
    "    # -----------❗ Core optimization: O(n), dictionary lookup with caching -----------\n",
    "    for w, is_proper in zip(words, is_proper_flags):\n",
    "\n",
    "        if is_proper:\n",
    "            # Proper nouns are automatically valid\n",
    "            continue\n",
    "\n",
    "        # Use cache to improve efficiency\n",
    "        if w in valid_cache:\n",
    "            is_valid = valid_cache[w]\n",
    "        else:\n",
    "            is_valid = (w in nlp.vocab)\n",
    "            valid_cache[w] = is_valid\n",
    "\n",
    "        if not is_valid:\n",
    "            invalid_words.append(w)\n",
    "\n",
    "    # ----------- Output results -----------\n",
    "    invalid_freq = Counter(invalid_words)\n",
    "    df_invalid = pd.DataFrame(\n",
    "        invalid_freq.items(), columns=[\"invalid_word\", \"frequency\"]\n",
    "    ).sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "    accuracy = (total - len(invalid_words)) / total\n",
    "\n",
    "    return accuracy, df_invalid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc856bdd",
   "metadata": {},
   "source": [
    "## 3.1 Baseline: Extract text directly by PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd06bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_direct_original(pdf_name,\n",
    "                                 pdf_dir=Path(\"data/speeches/pdf\")):\n",
    "    \"\"\"\n",
    "    for text-based PDFs, directly extract using PyMuPDF without going through k2pdfopt or OCR.\n",
    "    \"\"\"\n",
    "    pdf_path = pdf_dir / pdf_name\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "\n",
    "    for i, page in enumerate(doc, start=1):\n",
    "        text = page.get_text(\"text\")   # already machine-readable text\n",
    "        all_text.append(text)\n",
    "    doc.close()\n",
    "\n",
    "    full_text = \"\\n\".join(all_text)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a864c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_txt = extract_text_direct_original(\"A_1983_38_PV.81_speeches.pdf\")\n",
    "save_path = TMP_DIR / \"A_1983_38_PV.81_speeches_direct.txt\"\n",
    "\n",
    "# with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(direct_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9816512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\54241\\anaconda3\\envs\\ds_project\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9983934477870531\n",
      "Invalid words frequency:\n",
      "                    invalid_word  frequency\n",
      "6                        thearab          2\n",
      "19                     theunited          2\n",
      "0                             li          1\n",
      "37                      dyseized          1\n",
      "28                         ajust          1\n",
      "29                      thechair          1\n",
      "30                 aggressionsor          1\n",
      "31                        ofarab          1\n",
      "32                        peapie          1\n",
      "33                   whichisrael          1\n",
      "34                itsimpotencein          1\n",
      "35                      thegolan          1\n",
      "36                tochallengeits          1\n",
      "38                          wiii          1\n",
      "26                     thosearab          1\n",
      "39                      ofturkey          1\n",
      "40                        theplo          1\n",
      "41     attheconferenceemphasized          1\n",
      "42                 ofpalestinian          1\n",
      "43                chosepalestine          1\n",
      "44                thesecretaryof          1\n",
      "45              theunitedkingdom          1\n",
      "46                   betweenjune          1\n",
      "47                          ofte          1\n",
      "27                         smali          1\n",
      "24                 millionjewsin          1\n",
      "25                  mygovernment          1\n",
      "12                    thecharter          1\n",
      "2                           weil          1\n",
      "3                   exybnsionist          1\n",
      "4                      ofthearab          1\n",
      "5              byisraelisoldiers          1\n",
      "7                    forceisrael          1\n",
      "8                   occupiedarab          1\n",
      "9                  thequestionof          1\n",
      "10                  thepalestine          1\n",
      "11                   ofpalestine          1\n",
      "13                      peoplein          1\n",
      "1                            ifj          1\n",
      "14                           lod          1\n",
      "15                         lonal          1\n",
      "16                  mtransigence          1\n",
      "17      interpretationjromarabic          1\n",
      "18                      ofjewish          1\n",
      "20  theisraelientityunreservedly          1\n",
      "21                lienablerights          1\n",
      "22        defencelesspalestinian          1\n",
      "23                  directunited          1\n",
      "48       ofpresidentialdocuments          1\n"
     ]
    }
   ],
   "source": [
    "accuracy, df_invalid = evaluate_fast(direct_txt)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Invalid words frequency:\")\n",
    "print(df_invalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f4925",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
